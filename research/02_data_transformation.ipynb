{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Stock_Price_Prediction_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Stock_Price_Prediction_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    apple_data_dir: Path\n",
    "    amazon_data_dir: Path\n",
    "    google_data_dir: Path\n",
    "    microsoft_data_dir: Path\n",
    "    transformed_apple_data_dir: Path\n",
    "    transformed_amazon_data_dir: Path\n",
    "    transformed_google_data_dir: Path\n",
    "    transformed_microsoft_data_dir: Path\n",
    "    silk_data_dir: Path\n",
    "    pace_data_dir: Path\n",
    "    fauji_data_dir: Path\n",
    "    punjab_data_dir: Path\n",
    "    transformed_silk_data_dir: Path\n",
    "    transformed_pace_data_dir: Path\n",
    "    transformed_fauji_data_dir: Path\n",
    "    transformed_punjab_data_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smPredictor.constants import *\n",
    "from smPredictor.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            apple_data_dir=config.apple_data_dir,\n",
    "            amazon_data_dir=config.apple_data_dir,\n",
    "            google_data_dir=config.apple_data_dir,\n",
    "            microsoft_data_dir=config.apple_data_dir,\n",
    "            transformed_apple_data_dir= config.transformed_apple_data_dir,\n",
    "            transformed_amazon_data_dir=config.transformed_amazon_data_dir,\n",
    "            transformed_google_data_dir=config.transformed_google_data_dir,\n",
    "            transformed_microsoft_data_dir=config.transformed_microsoft_data_dir,\n",
    "            silk_data_dir=config.silk_data_dir,\n",
    "            pace_data_dir=config.pace_data_dir,\n",
    "            fauji_data_dir=config.fauji_data_dir,\n",
    "            punjab_data_dir=config.punjab_data_dir,\n",
    "            transformed_silk_data_dir=config.transformed_silk_data_dir,\n",
    "            transformed_pace_data_dir=config.transformed_pace_data_dir,\n",
    "            transformed_fauji_data_dir=config.transformed_fauji_data_dir,\n",
    "            transformed_punjab_data_dir=config.transformed_punjab_data_dir\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smPredictor import logger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def transform_data(self):\n",
    "        # Transformations for Apple data\n",
    "        apple_data = pd.read_csv(self.config.apple_data_dir)\n",
    "        apple_data = apple_data.iloc[1:]\n",
    "        apple_data.rename(columns={\"Price\": \"Date\"}, inplace=True)\n",
    "        apple_data = apple_data.drop(apple_data.index[0])\n",
    "        apple_data = apple_data.filter(['Close'])\n",
    "        apple_dataset = apple_data.values\n",
    "        apple_training_data_len = int(np.ceil(len(apple_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        apple_scaled_data = scaler.fit_transform(apple_dataset)\n",
    "        apple_train_data = apple_scaled_data[:apple_training_data_len]\n",
    "        apple_train_data_df = pd.DataFrame(apple_train_data)\n",
    "        os.makedirs(self.config.transformed_apple_data_dir, exist_ok=True)\n",
    "        apple_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_apple_data_dir, \"transformed_apple.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Apple Data saved successfully\")\n",
    "        print(f\"Apple data shape: {apple_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Amazon Data\n",
    "        amazon_data = pd.read_csv(self.config.amazon_data_dir)\n",
    "        amazon_data = amazon_data.iloc[1:]\n",
    "        amazon_data.rename(columns={\"Price\": \"Date\"}, inplace=True)\n",
    "        amazon_data = amazon_data.drop(amazon_data.index[0])\n",
    "        amazon_data = amazon_data.filter(['Close'])\n",
    "        amazon_dataset = amazon_data.values\n",
    "        amazon_training_data_len = int(np.ceil(len(amazon_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        amazon_scaled_data = scaler.fit_transform(amazon_dataset)\n",
    "        amazon_train_data = amazon_scaled_data[:amazon_training_data_len]\n",
    "        amazon_train_data_df = pd.DataFrame(amazon_train_data)\n",
    "        os.makedirs(self.config.transformed_amazon_data_dir, exist_ok=True)\n",
    "        amazon_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_amazon_data_dir, \"transformed_amazon.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Amazon Data saved successfully\")\n",
    "        print(f\"Amazon data shape: {amazon_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Google Data\n",
    "        google_data = pd.read_csv(self.config.google_data_dir)\n",
    "        google_data = google_data.iloc[1:]\n",
    "        google_data.rename(columns={\"Price\": \"Date\"}, inplace=True)\n",
    "        google_data = google_data.drop(google_data.index[0])\n",
    "        google_data = google_data.filter(['Close'])\n",
    "        google_dataset = google_data.values\n",
    "        google_training_data_len = int(np.ceil(len(google_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        google_scaled_data = scaler.fit_transform(google_dataset)\n",
    "        google_train_data = google_scaled_data[:google_training_data_len]\n",
    "        google_train_data_df = pd.DataFrame(google_train_data)\n",
    "        os.makedirs(self.config.transformed_google_data_dir, exist_ok=True)\n",
    "        google_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_google_data_dir, \"transformed_google.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Google Data saved successfully\")\n",
    "        print(f\"Google data shape: {google_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Microsoft Data\n",
    "        microsoft_data = pd.read_csv(self.config.microsoft_data_dir)\n",
    "        microsoft_data = microsoft_data.iloc[1:]\n",
    "        microsoft_data.rename(columns={\"Price\": \"Date\"}, inplace=True)\n",
    "        microsoft_data = microsoft_data.drop(microsoft_data.index[0])\n",
    "        microsoft_data = microsoft_data.filter(['Close'])\n",
    "        microsoft_dataset = microsoft_data.values\n",
    "        microsoft_training_data_len = int(np.ceil(len(microsoft_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        microsoft_scaled_data = scaler.fit_transform(microsoft_dataset)\n",
    "        microsoft_train_data = microsoft_scaled_data[:microsoft_training_data_len]\n",
    "        microsoft_train_data_df = pd.DataFrame(microsoft_train_data)\n",
    "        os.makedirs(self.config.transformed_microsoft_data_dir, exist_ok=True)\n",
    "        microsoft_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_microsoft_data_dir, \"transformed_microsoft.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Microsoft Data saved successfully\")\n",
    "        print(f\"Microsoft data shape: {microsoft_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Silk Bank Data\n",
    "        silk_data = pd.read_csv(self.config.silk_data_dir)\n",
    "        silk_data = silk_data.filter(['Close'])\n",
    "        silk_dataset = silk_data.values\n",
    "        silk_training_data_len = int(np.ceil(len(silk_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        silk_scaled_data = scaler.fit_transform(silk_dataset)\n",
    "        silk_train_data = silk_scaled_data[:silk_training_data_len]\n",
    "        silk_train_data_df = pd.DataFrame(silk_train_data)\n",
    "        os.makedirs(self.config.transformed_silk_data_dir, exist_ok=True)\n",
    "        silk_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_silk_data_dir, \"transformed_silk.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Silk Data saved successfully\")\n",
    "        print(f\"Silk data shape: {silk_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Pace Pakistan Data\n",
    "        pace_data = pd.read_csv(self.config.pace_data_dir)\n",
    "        pace_data = pace_data.filter(['Close'])\n",
    "        pace_dataset = pace_data.values\n",
    "        pace_training_data_len = int(np.ceil(len(pace_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        pace_scaled_data = scaler.fit_transform(pace_dataset)\n",
    "        pace_train_data = pace_scaled_data[:pace_training_data_len]\n",
    "        pace_train_data_df = pd.DataFrame(pace_train_data)\n",
    "        os.makedirs(self.config.transformed_pace_data_dir, exist_ok=True)\n",
    "        pace_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_pace_data_dir, \"transformed_pace.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Pace Data saved successfully\")\n",
    "        print(f\"Pace data shape: {pace_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Fauji Foods Limited Data\n",
    "        fauji_data = pd.read_csv(self.config.fauji_data_dir)\n",
    "        fauji_data = fauji_data.filter(['Close'])\n",
    "        fauji_dataset = fauji_data.values\n",
    "        fauji_training_data_len = int(np.ceil(len(fauji_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        fauji_scaled_data = scaler.fit_transform(fauji_dataset)\n",
    "        fauji_train_data = fauji_scaled_data[:fauji_training_data_len]\n",
    "        fauji_train_data_df = pd.DataFrame(fauji_train_data)\n",
    "        os.makedirs(self.config.transformed_fauji_data_dir, exist_ok=True)\n",
    "        fauji_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_fauji_data_dir, \"transformed_fauji.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Fauji Data saved successfully\")\n",
    "        print(f\"Fauji data shape: {fauji_train_data_df.shape}\")\n",
    "        \n",
    "        # Transformation for Bank of Punjab Data\n",
    "        punjab_data = pd.read_csv(self.config.punjab_data_dir)\n",
    "        punjab_data = punjab_data.filter(['Close'])\n",
    "        punjab_dataset = punjab_data.values\n",
    "        punjab_training_data_len = int(np.ceil(len(punjab_dataset) * 0.95))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        punjab_scaled_data = scaler.fit_transform(punjab_dataset)\n",
    "        punjab_train_data = punjab_scaled_data[:punjab_training_data_len]\n",
    "        punjab_train_data_df = pd.DataFrame(punjab_train_data)\n",
    "        os.makedirs(self.config.transformed_punjab_data_dir, exist_ok=True)\n",
    "        punjab_train_data_df.to_csv(\n",
    "            os.path.join(self.config.transformed_punjab_data_dir, \"transformed_punjab.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        logger.info(\"Transformed Punjab Data saved successfully\")\n",
    "        print(f\"Punjab data shape: {punjab_train_data_df.shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-30 18:59:36,339: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-30 18:59:36,349: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-11-30 18:59:36,351: INFO: common: created directory at: artifacts]\n",
      "[2024-11-30 18:59:36,351: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-11-30 18:59:36,461: INFO: 536690641: Transformed Apple Data saved successfully]\n",
      "Apple data shape: (3087, 1)\n",
      "[2024-11-30 18:59:36,633: INFO: 536690641: Transformed Amazon Data saved successfully]\n",
      "Amazon data shape: (3087, 1)\n",
      "[2024-11-30 18:59:36,739: INFO: 536690641: Transformed Google Data saved successfully]\n",
      "Google data shape: (3087, 1)\n",
      "[2024-11-30 18:59:36,833: INFO: 536690641: Transformed Microsoft Data saved successfully]\n",
      "Microsoft data shape: (3087, 1)\n",
      "[2024-11-30 18:59:36,874: INFO: 536690641: Transformed Silk Data saved successfully]\n",
      "Silk data shape: (3040, 1)\n",
      "[2024-11-30 18:59:36,919: INFO: 536690641: Transformed Pace Data saved successfully]\n",
      "Pace data shape: (3038, 1)\n",
      "[2024-11-30 18:59:36,949: INFO: 536690641: Transformed Fauji Data saved successfully]\n",
      "Fauji data shape: (1976, 1)\n",
      "[2024-11-30 18:59:36,979: INFO: 536690641: Transformed Punjab Data saved successfully]\n",
      "Punjab data shape: (3040, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.transform_data()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
